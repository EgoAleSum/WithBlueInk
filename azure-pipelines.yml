# CI pipelines for Azure Pipelines, using Hugo

# Define repository
resources:
  - repo: self

# Trigger on changes to the master branch
trigger:
  - master+

# Variables for the entire pipeline
variables:
  # Version of Hugo
  HUGO_VERSION: 0.56.3
  # Version of Pinata Pinner
  PINATAPINNER_VERSION: 0.1.1
  # Domain to update (DNSLink)
  CLOUDFLARE_DOMAIN: '_dnslink.withblue.ink'

# Stages
stages:

  # Build site stage
  - stage: build
    displayName: 'Build site'
    jobs:

    # Build site job
      - job: build
        displayName: 'Build site'

        # Run on Linux (Ubuntu 16.04)
        pool:
          vmImage: 'Ubuntu-16.04'

        # Steps
        steps:

          # Install Node.js first, which is necessary for the module depdendencies
          - task: NodeTool@0
            displayName: 'Use Node.js 10.x'
            inputs:
              versionSpec: 10.x

          # Install Hugo
          - script: |
              set -e
              cd /tmp
              echo "Using Hugo $(HUGO_VERSION)"
              curl -fsSL "https://github.com/gohugoio/hugo/releases/download/v$(HUGO_VERSION)/hugo_extended_$(HUGO_VERSION)_Linux-64bit.tar.gz" -o hugo.tar.gz
              tar -zxf hugo.tar.gz
              sudo mv hugo /usr/local/bin
            displayName: 'Install Hugo'

          # Build the site
          - script: 'make dist'
            displayName: 'Build site'

          # Publish the compiled site as pipeline artifact
          - publish: '$(System.DefaultWorkingDirectory)/public'
            displayName: 'Publish Artifact: public'
            artifact: 'public'

  # Stage app stage
  - stage: staging
    displayName: 'Stage app'
    jobs:

      # Stage app job
      - deployment: staging
        environment: Staging
        displayName: 'Stage app'

        # Run on Linux (Ubuntu 16.04)
        pool:
          vmImage: 'Ubuntu-16.04'
        
        # Deployment strategy is "runOnce"
        strategy:
          runOnce:
            deploy:
              # Steps
              steps:
                # Download pipeline artifact
                - download: current
                  artifact: 'public'

                # Copy files to staging via SSH
                - task: CopyFilesOverSSH@0
                  displayName: 'Copy files to staging via SSH'
                  inputs:
                    sshEndpoint: 'IPFS-EU'
                    sourceFolder: '$(Pipeline.Workspace)/public'
                    targetFolder: '/data/ipfs-staging/withblueink'
                    cleanTargetFolder: true
                    failOnEmptySource: true

                # Pin files on IPFS
                - task: SSH@0
                  name: pinfiles
                  displayName: 'Pin files on IPFS'
                  inputs:
                    sshEndpoint: 'IPFS-EU'
                    runOptions: inline
                    inline: |
                      set -e
                      
                      # Add files to IPFS
                      HASH=$(sudo docker exec ipfs-node ipfs add -rQ /staging/withblueink)
                      echo $HASH
                      echo "##vso[task.setvariable variable=IPFS_HASH;isOutput=true]$HASH"
                      # Add to the pinset of the cluster
                      sudo docker exec ipfs-cluster ipfs-cluster-ctl pin add $HASH

                # Pin on Pinata
                - script: |
                    set -e
                    
                    # Download pinatapinner
                    sudo curl -L "https://github.com/ItalyPaleAle/pinatapinner/releases/download/v$(PINATAPINNER_VERSION)/pinatapinner_linux_amd64" -o /usr/local/bin/pinatapinner
                    sudo chmod +x /usr/local/bin/pinatapinner
                    
                    # Pin on Pinata
                    pinatapinner $(Pipeline.Workspace)/public "WithBlueInk-${RELEASE_RELEASEID}"
                  displayName: 'Pin on Pinata'
                  env:
                    PINATA_API_KEY: $(PINATA_API_KEY)
                    PINATA_SECRET_KEY: $(PINATA_SECRET_KEY)

  # Deploy to production stage
  - stage: production
    displayName: 'Deploy to production'
    jobs:

      # Deploy to production
      - deployment: production
        environment: Production
        displayName: 'Deploy to production'

        # Run on Linux (Ubuntu 16.04)
        pool:
          vmImage: 'Ubuntu-16.04'
        
        # Link with variable IPFS_HASH from previous job
        variables:
          IPFS_HASH: $[ dependencies.staging.outputs['pinfiles.IPFS_HASH'] ]
        
        # Deployment strategy is "runOnce"
        strategy:
          runOnce:
            deploy:
              # Steps
              steps:
                - script: |
                    set -e
                    
                    echo "Updating DNSLink to ${IPFS_HASH}"

                    RECORD_ID=$(curl -sS -X GET "https://api.cloudflare.com/client/v4/zones/$(CLOUDFLARE_ZONE_ID)/dns_records?type=TXT&name=$(CLOUDFLARE_DOMAIN)" \
                          -H "Content-Type:application/json" \
                          -H "X-Auth-Key:$(CLOUDFLARE_API_KEY)" \
                          -H "X-Auth-Email:$(CLOUDFLARE_EMAIL)" \
                              | jq -r '.result[0].id')
                    
                    curl -sS -X PUT "https://api.cloudflare.com/client/v4/zones/$(CLOUDFLARE_ZONE_ID)/dns_records/$RECORD_ID" \
                          -H "Content-Type:application/json" \
                          -H "X-Auth-Key:$(CLOUDFLARE_API_KEY)" \
                          -H "X-Auth-Email:$(CLOUDFLARE_EMAIL)" \
                          --data "{\"type\":\"TXT\",\"name\":\"$(CLOUDFLARE_DOMAIN)\",\"content\":\"dnslink=/ipfs/$IPFS_HASH\",\"ttl\":120,\"priority\":10,\"proxied\":false}"
                  displayName: 'Update DNS on Cloudflare'
                  env:
                    IPFS_HASH: $(IPFS_HASH)
